<!doctype html><html><head lang=en><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Sklearn Pipelines - Blog</title><link rel=icon type=image/png href=img/favicon.ico><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="I tried to illustrate the usage of scikit-learn pipelines."><meta property="og:image" content><meta property="og:url" content="https://kevinkle.in/posts/2019-01-10-sklearn-pipelines/"><meta property="og:site_name" content="Blog"><meta property="og:title" content="Sklearn Pipelines"><meta property="og:description" content="I tried to illustrate the usage of scikit-learn pipelines."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-01-08T13:38:07+02:00"><meta property="article:modified_time" content="2019-01-08T13:38:07+02:00"><meta property="article:tag" content="Tech"><meta name=twitter:card content="summary"><meta name=twitter:title content="Sklearn Pipelines"><meta name=twitter:description content="I tried to illustrate the usage of scikit-learn pipelines."><script src=https://kevinkle.in/js/feather.min.js></script><link href=https://kevinkle.in/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://kevinkle.in/css/main.cd139160ccd0ca6bf7d891fb9f590f6c1f70199ff96cd189099b67d9081114e9.css></head><body><div class=content><header><div class=main><a href=https://kevinkle.in/>Blog</a></div><nav><a href=/about>Hello</a>
<a href=/tags>Tags</a>
<a href=/projects>Projects</a></nav></header><main><article><div class=title><h1 class=title>Sklearn Pipelines</h1><div class=meta>Posted on Jan 8, 2019</div></div><section class=body><blockquote><p>Pipeline of transforms with a final estimator.</p></blockquote><h2 id=why>Why</h2><p>Whether you&rsquo;re using sklearn transformations already or implementing data wrangling from scratch, there are interesting aspects to squeezing those into an sklearn
<a href=https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html>pipeline</a>.
Most notably, this will:</p><ul><li>Allow for the usage of sklearn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html>cross validation</a>.</li><li>Allow for the usage of sklearn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html>grid search</a>.</li><li>Lower the barrier to include sklearn transformations, such as <a href=https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html>TfidfVectorizer</a>, in the future.</li></ul><p>It would be possible to do all such processing without a pipeline object: once
on the whole dataset. Yet, this is only legitimate for data-independent [0]
transformations. For data-dependent transformations, such as imputation by mean,
you would cause data leakage [1]. In order to avoid that, we can express how
training and validation set should be processed individually, per cross
validation split.</p><p>This distinction is manifested through <code>fit</code> and <code>transform</code> methods in sklearn. For data-dependent operations, <code>fit</code> is supposed to capture said
dependence. <code>fit</code> is therefore only applied to training data while <code>transform</code> is applied to both training and validation data. Moreover it follows that <code>fit</code> for data-dependent operations is stateful.</p><h2 id=pipeline-building-blocks>Pipeline building blocks</h2><p>A pipeline of n steps constitutes of n-1 transforms and a final estimator.</p><h1 id=transform-steps>Transform steps</h1><p>In order to define a transform step, <code>fit</code> and <code>transform</code> have to be implemented. Conventionally, this is
achieved by inherting from sklearn&rsquo;s <a href=https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html>TransformerMixin</a>.</p><p>Given that the pipeline is fed a pandas dataframe, the following are examples
of sequential transform step classes:</p><ol><li>Data-indepdent imputation.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DateImputer</span>(TransformerMixin):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, df):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Observations don&#39;t always exist. Hence impute by lower/upper bound.</span>
</span></span><span style=display:flex><span>        df <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>fillna({<span style=color:#e6db74>&#39;date_first_observation&#39;</span>: df[<span style=color:#e6db74>&#39;date_creation&#39;</span>],
</span></span><span style=display:flex><span>                        <span style=color:#e6db74>&#39;date_last_observation&#39;</span>: df[<span style=color:#e6db74>&#39;date_deletion&#39;</span>]})
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> df
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, df, labels<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span></code></pre></div><ol start=2><li>Data-independent creation of feature.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RelativeWeightCreator</span>(TransformerMixin):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, df):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> column_name <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#39;weight_a&#39;</span>, <span style=color:#e6db74>&#39;weight_b&#39;</span>, <span style=color:#e6db74>&#39;weight_c&#39;</span>]:
</span></span><span style=display:flex><span>            df<span style=color:#f92672>.</span>loc[:, <span style=color:#e6db74>&#34;rel&#34;</span> <span style=color:#f92672>+</span> column_name] <span style=color:#f92672>=</span> df[column_name] <span style=color:#f92672>/</span> df[<span style=color:#e6db74>&#39;total_weight&#39;</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> df
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, df, labels<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span></code></pre></div><ol start=3><li>Data-dependent creation of feature.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AggregateCreator</span>(TransformerMixin):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>asset_mean_dict <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, df, labels<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        outcome <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;profit&#39;</span>
</span></span><span style=display:flex><span>        mask <span style=color:#f92672>=</span> df[outcome] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        df <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>loc[mask, :]
</span></span><span style=display:flex><span>        asset_aggregates <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>groupby(<span style=color:#e6db74>&#39;asset_type&#39;</span>)[outcome]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>asset_mean_dict <span style=color:#f92672>=</span> asset_aggregates<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>to_dict()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, df):
</span></span><span style=display:flex><span>        asset_types <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;asset_types&#39;</span>]
</span></span><span style=display:flex><span>        asset_means <span style=color:#f92672>=</span> asset_types<span style=color:#f92672>.</span>map(self<span style=color:#f92672>.</span>asset_mean_dict)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># For validation data, some asset_type values might not have been</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># encountered in training data.</span>
</span></span><span style=display:flex><span>        asset_means <span style=color:#f92672>=</span> asset_means<span style=color:#f92672>.</span>fillna(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        df<span style=color:#f92672>.</span>loc[:, <span style=color:#e6db74>&#39;asset_mean&#39;</span>] <span style=color:#f92672>=</span> asset_means
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> df
</span></span></code></pre></div><ol start=4><li>Data-independent dropping of columns.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ColumnDropper</span>(TransformerMixin):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, column_names):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>column_names <span style=color:#f92672>=</span> column_names
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, df):
</span></span><span style=display:flex><span>        df <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>drop(self<span style=color:#f92672>.</span>column_names, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> df
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, df, labels<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span></code></pre></div><p>Many more use cases exist.</p><h1 id=meta-information-step>Meta information step</h1><p>Steps in a pipeline don&rsquo;t <em>have</em> to <em>actually</em> process or estimate. See an example of a pipeline step which prints some stats about the data before it&rsquo;s being handed to the regressor. In addition, it stores column name information, which would not be obtainable by default. This is arguably hacky.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MetaInformer</span>(TransformerMixin):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>column_names <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>transform</span>(self, df):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#39;#columns: </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>%</span> df<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>        column_nullity <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>isnull()<span style=color:#f92672>.</span>any()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> column_nullity<span style=color:#f92672>.</span>any():
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#39;Contain nulls:&#39;</span>)
</span></span><span style=display:flex><span>            print(column_nullity[<span style=color:#66d9ef>lambda</span> x: x]<span style=color:#f92672>.</span>keys())
</span></span><span style=display:flex><span>        column_redundancy <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>apply(<span style=color:#66d9ef>lambda</span> c: c<span style=color:#f92672>.</span>min() <span style=color:#f92672>==</span> c<span style=color:#f92672>.</span>max(), axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> column_redundancy<span style=color:#f92672>.</span>any():
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#39;Has redundant columns:&#39;</span>)
</span></span><span style=display:flex><span>            print(column_redundancy[<span style=color:#66d9ef>lambda</span> x: x]<span style=color:#f92672>.</span>keys())
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> df
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, df, labels<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>column_names <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>columns
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_column_names</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>column_names
</span></span></code></pre></div><h1 id=estimation-step>Estimation step</h1><p>We can simply use an out-of-the-box estimator from sklearn, such as <a href=https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html>RandomForestRegressor</a>.
If you intend to either eliminate rows (requiring access to the target dataframe) or want to still operate on predictions, it is possible to wrap a conventional estimator inside of a custom one. The latter case might, for instance, be relevant if you have a strong belief on how a target variable is constructed off of intermediate target variables.</p><p>Assuming you happen to know that profit/square meters is &rsquo;easier&rsquo; to infer than profits, you could manually build an interaction term multiplying the inferred profits per surface area by surface area.
If you want this type of post-inference-processing to work with
GridSearchCV, you need to implement the <code>set_params</code> method.</p><p>Applying a logarithm to the predictions is not inherent to the approach of building the interaction term. It is an independent design choice that can be conveniently placed at this stage of the pipeline.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>InteractionRegressor</span>(BaseEstimator):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, use_lb, use_log_on_savings):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>regressor <span style=color:#f92672>=</span> RandomForestRegressor()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>intermediate_target_column <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;profit_per_sqm&#39;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>max_depth <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_estimators <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, df_wo_pseudo_target, df_pseudo_target):
</span></span><span style=display:flex><span>        df <span style=color:#f92672>=</span> df_wo_pseudo_target
</span></span><span style=display:flex><span>        <span style=color:#75715e># Split target from samples.</span>
</span></span><span style=display:flex><span>        intermediate_target <span style=color:#f92672>=</span> df[self<span style=color:#f92672>.</span>intermediate_target_column]
</span></span><span style=display:flex><span>        df_wo_intermediate_target <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>drop(self<span style=color:#f92672>.</span>intermediate_target_column,
</span></span><span style=display:flex><span>                                            axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        intermediate_target_log <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>log(intermediate_target)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>regressor<span style=color:#f92672>.</span>fit(df_wo_intermediate_target, intermediate_target_log)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, df):
</span></span><span style=display:flex><span>        df_wo_intermediate_target <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>drop(self<span style=color:#f92672>.</span>intermediate_target_column,
</span></span><span style=display:flex><span>                                            axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        surface_areas <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;surface_area&#39;</span>]
</span></span><span style=display:flex><span>        intermediate_target_log <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>regressor<span style=color:#f92672>.</span>predict(
</span></span><span style=display:flex><span>          df_wo_intermediate_target)
</span></span><span style=display:flex><span>        <span style=color:#75715e># log of product is sum of logs.</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> intermediate_target_log <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>log(surface_areas)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_rf_params</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#39;max_depth&#39;</span>: self<span style=color:#f92672>.</span>max_depth,
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#39;n_estimators&#39;</span>: self<span style=color:#f92672>.</span>n_estimators}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>set_params</span>(self, <span style=color:#f92672>**</span>params):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>set_params(<span style=color:#f92672>**</span>params)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>regressor<span style=color:#f92672>.</span>set_params(<span style=color:#f92672>**</span>self<span style=color:#f92672>.</span>get_rf_params())
</span></span></code></pre></div><h2 id=instantiating-a-pipeline>Instantiating a pipeline</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># from sklearn.ensemble import RandomForestRegressor</span>
</span></span><span style=display:flex><span><span style=color:#75715e># from sklearn.pipeline import Pipeline</span>
</span></span><span style=display:flex><span>weight_columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;weight_a&#39;</span>, <span style=color:#e6db74>&#39;weight_b&#39;</span>, <span style=color:#e6db74>&#39;weight_c&#39;</span>]
</span></span><span style=display:flex><span>redundant_columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;useless&#39;</span>, <span style=color:#e6db74>&#39;noise&#39;</span>, <span style=color:#e6db74>&#39;garbage&#39;</span>]
</span></span><span style=display:flex><span>target_columns <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;revenue&#39;</span>, <span style=color:#e6db74>&#39;profit&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pipeline <span style=color:#f92672>=</span> Pipeline([
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;date_imputer&#39;</span>, DateImputer()),
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;relative_weight_creator&#39;</span>, RelativeWeightCreator()),
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;aggregate_creator&#39;</span>, AggregateCreator()),
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;weight_column_dropper&#39;</span>, ColumnDropper(weight_columns)),
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;redundant_column_dropper&#39;</span>, ColumnDropper(redundant_columns)),
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;target_column_dropper&#39;</span>, ColumnDropper(target_columns)),
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;meta_informer&#39;</span>, MetaInformer())
</span></span><span style=display:flex><span>  (<span style=color:#e6db74>&#39;rf_regressor&#39;</span>, RandomForestRegressor())
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><h2 id=using-a-pipeline>Using a pipeline</h2><h1 id=process-obtain-meta-information-and-predict>Process, obtain meta information and predict</h1><p>Note that in order for the feature importance information to be meaningful, we first need to retrieve the column names, stored in the <code>'meta_informer'</code> step of the pipeline.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>pipeline<span style=color:#f92672>.</span>fit(df_wo_target, df_target)
</span></span><span style=display:flex><span>column_names <span style=color:#f92672>=</span> pipeline<span style=color:#f92672>.</span>named_steps[<span style=color:#e6db74>&#39;meta_informer&#39;</span>]<span style=color:#f92672>.</span>get_column_names()
</span></span><span style=display:flex><span>feature_importances <span style=color:#f92672>=</span> pipeline<span style=color:#f92672>.</span>named_steps[<span style=color:#e6db74>&#39;rf_regressor&#39;</span>]<span style=color:#f92672>.</span>feature_importances_
</span></span><span style=display:flex><span>name_importance_tuples <span style=color:#f92672>=</span> sorted(
</span></span><span style=display:flex><span>  [(column_names[i], x) <span style=color:#66d9ef>for</span> i, x <span style=color:#f92672>in</span> enumerate(feature_importances) <span style=color:#66d9ef>if</span> x <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>  key<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> p: p[<span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>  reverse<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>print(name_importance_tuples)
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> pipeline<span style=color:#f92672>.</span>predict(df_target)
</span></span></code></pre></div><h1 id=cross-validation>Cross validation</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># from sklearn.model_selection import cross_val_score</span>
</span></span><span style=display:flex><span>cv_scores <span style=color:#f92672>=</span> cross_val_score(pipeline, df_data, df_target, cv<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>,
</span></span><span style=display:flex><span>                            scoring<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;neg_mean_squared_error&#39;</span>)
</span></span><span style=display:flex><span>avg_rmse <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(np<span style=color:#f92672>.</span>sqrt(<span style=color:#f92672>-</span>cv_scores))
</span></span><span style=display:flex><span>print(avg_rmse)
</span></span></code></pre></div><h1 id=grid-search>Grid search</h1><p>In order to associate a hyperparameter to a certain step, you need to prefix the hyperparameter attribute by the name of the pipeline
step. The postfix of the hyperparameter attribute is the parameter
said step of the pipeline expects, as used in <code>set_params</code>. Both parts are
joined via a double underscore.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># from sklearn.model_selection import GridSearchCV</span>
</span></span><span style=display:flex><span>hyperparameters <span style=color:#f92672>=</span> [{<span style=color:#e6db74>&#39;rf_regressor__max_depth&#39;</span>: [<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>15</span>, <span style=color:#ae81ff>25</span>],
</span></span><span style=display:flex><span>                    <span style=color:#e6db74>&#39;rf_regressor__n_estimators&#39;</span>: [<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>200</span>, <span style=color:#ae81ff>300</span>]}]
</span></span><span style=display:flex><span>grid_search <span style=color:#f92672>=</span> GridSearchCV(pipeline, hyperparameters)
</span></span><span style=display:flex><span>grid_search<span style=color:#f92672>.</span>fit(df_data, df_target)
</span></span><span style=display:flex><span>print(np<span style=color:#f92672>.</span>sqrt(<span style=color:#f92672>-</span>grid_search<span style=color:#f92672>.</span>best_score_))
</span></span><span style=display:flex><span>print(grid_search<span style=color:#f92672>.</span>best_params_)
</span></span></code></pre></div><p>[0] Data-dependent: If <code>f</code> is applied to sample <code>xi</code> out of all n samples, the outcome is independent of all other n-1 samples.</p><p>[1] <a href=https://machinelearningmastery.com/data-leakage-machine-learning/>https://machinelearningmastery.com/data-leakage-machine-learning/</a></p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/tech>tech</a></li></ul></nav></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/kklein rel=me title=GitHub><i data-feather=github></i></a>
<a class=border></a><a class=soc href=https://twitter.com/kevkle rel=me title=Twitter><i data-feather=twitter></i></a>
<a class=border></a></div><div class=footer-info>2024 © Kevin Klein |</div></footer><script async src="https://www.googletagmanager.com/gtag/js?id=G-SGRSG6Y0WX"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SGRSG6Y0WX")}</script><script>feather.replace()</script></div></body></html>