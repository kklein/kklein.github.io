<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>A Beautiful, Derandomized Algorithm - Blog</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="I tried to illustrate the process of turning an already useful randomized algorithm into a conventional, non-random algorithm."><meta property="og:image" content><meta property="og:title" content="A Beautiful, Derandomized Algorithm"><meta property="og:description" content="I tried to illustrate the process of turning an already useful randomized algorithm into a conventional, non-random algorithm."><meta property="og:type" content="article"><meta property="og:url" content="https://kevinkle.in/posts/2019-01-22-favourite-algorithm/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-01-21T16:38:07+02:00"><meta property="article:modified_time" content="2019-01-21T16:38:07+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Beautiful, Derandomized Algorithm"><meta name=twitter:description content="I tried to illustrate the process of turning an already useful randomized algorithm into a conventional, non-random algorithm."><script src=https://kevinkle.in/js/feather.min.js></script>
<link href=https://kevinkle.in/css/fonts.b685ac6f654695232de7b82a9143a46f9e049c8e3af3a21d9737b01f4be211d1.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://kevinkle.in/css/main.d444e443bc249bae3ac5015ff4d1a36840bb33e18c03983805f61fd4ad5e17f0.css></head><body><div class=content><header><div class=main><a href=https://kevinkle.in/>Blog</a></div><nav><a href=/about>Hello</a>
<a href=/tags>Tags</a>
<a href=/projects>Projects</a>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script></nav></header><main><article><div class=title><h1 class=title>A Beautiful, Derandomized Algorithm</h1><div class=meta>Posted on Jan 21, 2019</div></div><section class=body><h3 id=abstract>Abstract</h3><p>Appreciating the simplicity of powerful randomized algorithms comes easy to me. In the following I will try to describe a naive algorithm relying on a randomized argument. What I find particularly remarkable is that it can easily parry the critique I am so often encountered with when cherishing randomized algorithms: it can trivially be turned deterministic.</p><h3 id=max-3sat>MAX-3SAT</h3><p>Is a boolean satisfiability problem. Given a formula in 3-CNF, the goal to satisfy as many clauses as possible.</p><p>3-CNF means that the formula \(\mathcal{F}\) consists of \(m\) conjuncted clauses using \(n\) literals. Each clause is a disjunction of exactly three literals. An example with \(m=4\) and \(n=5\):</p><p>\[\begin{aligned}
\mathcal{F} &= C_1 \wedge C_2 \wedge C_3 \wedge C_4 \\
&= (l_1 \vee \neg l_2 \vee l_4) \wedge (l_2 \vee l_4 \vee \neg l_5) \wedge (\neg l_1 \vee \neg l_3 \vee l_5) \wedge (l_2 \vee l_3 \vee l_4)
\end{aligned}\]</p><p>This problem is \(\mathcal{NP}\)-hard and we will look into an approximation algorithm.</p><h3 id=at-least-how-many-clauses-can-be-satisfied-by-any-algorithm>At least how many clauses can be satisfied, by any algorithm?</h3><p>In order to show that <em>at least a lot</em> of the clauses can be satisfied for every formula, we employ the <em>probabilistic method</em>.</p><p>Let&rsquo;s define a randomized algorithm.</p><p>Each literal \(l_i\) is i.i.d. from the
uniform distribution over \(\{True, False\}\). This turns every clause, and thereby the number of satisfied clauses \(M\) into a random variable.</p><p>We observe that a clause is satisfied if any of is components, being either a literal or a negated literal, is \(True\). Hence it is \(True\) in all cases but the one in which all of its components are \(False\).</p><p>\[\Pr[C_i] = 1 - \Pr[\text{all components are }False] = 1 - \frac{1}{2}^3 = \frac{7}{8}\]</p><p>We need no more but our dearest linearity of expectation to show that for every formula, we satisfy \(\frac{7}{8}\) of its clauses, <em>in expectaion</em>.</p><p>\[\begin{aligned}
\mathbb{E}[M] &= \mathbb{E}[\sum_{i=1}^m C_i] \\
&= \sum_{i=1}^m \mathbb{E}[C_i] = \sum_{i=1}^m \Pr[C_i] \quad \text{(L.O.E.)} \\
&= \frac{7}{8}m
\end{aligned}\]</p><p>The <em>probabilistic method</em> allows us to draw an interesting conclusion from this. We have shown that \(\frac{7}{8}\) of all clauses are satisfied <em>in expectation</em> for a given formula. As the expected value is a sum of non-negative values, we know that one of the realizations the expected value sums over has to take on the value of the expectation, i.e. \(\frac{7}{8}m\). Hence one of the possible assignments will lead to the satisfaction of at least \(\frac{7}{8}m\) clauses, for every formula.</p><p>I find that this by itself is an interesting, not necessarily intuitive result. It is a proof of existence.</p><p>The probabilistic method is often used for proofs of existence or/i.e. non-zero success probabilities of randomized algorithms. In this particular scenario we will also use its insight for the synthesis of a deterministic algorithm. The plan is to assign truth values to literals as to approximate the maximum assignment. This approach is sometimes referred to as derandomization via <em>conditional expectation</em>.</p><h3 id=construction-of-deterministic-algorithm>Construction of deterministic algorithm</h3><p>The high-level idea of the algorithm is that we look at one literal after another, calculate the expected value conditioned on a possible assignment and then set this literal to the truth value that yields the highest expected value.</p><p>The algorithm relies on two key properties:</p><ul><li>We don&rsquo;t decrease the (conditional) expectation of \(M\) by conditioning on the assignment that maximizes the conditional expectation.</li><li>We can compute the conditional expectation &rsquo;efficiently'.</li></ul><h4 id=increasing-expectation>Increasing expectation</h4><p>Our random assignment process used to compute the expected value tells us that:
\[\mathbb{E}[M|l_1=\alpha_1, &mldr;, l_{i-1}=\alpha_{i-1}] = \\
\frac{1}{2} \mathbb{E}[M|l_1=\alpha_1, &mldr;, l_{i-1}=\alpha_{i-1}, l_i=0] + \frac{1}{2}
\mathbb{E}[M|l_1=\alpha_1, &mldr;, l_{i-1}=\alpha_{i-1}, l_i=1]\]</p><p>\[ \Rightarrow max(\mathbb{E}[M|l_1=\alpha_1, &mldr;, l_{i-1}=\alpha_{i-1}, l_i=0],
\mathbb{E}[M|l_1=\alpha_1, &mldr;, l_{i-1}=\alpha_{i-1}, l_i=1]) \\
\geq \mathbb{E}[M|l_1=\alpha_1, &mldr;, l_{i-1}=\alpha_{i-1}]
\]</p><p>Therefore we know that <em>iteratively</em> setting literals to the truth value maximizing the conditional expectation will always give us \(\mathbb{E}[M]|l=\alpha] \geq \mathbb{E}[M] = \frac{7}{8}m\).</p><h4 id=cost-of-computing-conditional-expectation>Cost of computing conditional expectation</h4><p>Given assignments \(l_1 = \alpha_1, \dots, l_i = \alpha_i\), computing \(\mathbb{E}[M|l_1 = \alpha_1, \dots, l_i = \alpha_i]\) is actually pretty easy. We go clause by clause, each containing 3 literals and hence requiring \(\mathcal{O}(1)\) time. If all literals of a clause are determined, we check whether the clause is satisfied or not and accordingly count it as 0 or 1. If some of the literals are set, we add the probability of it being satisfied, e.g. \(\Pr[False \vee l_{i+1} \vee \neg l_{i+2}] = 1 - \frac{1}{4} = \frac{3}{4}\). All clauses that have no determined literals keep their expected satisfaction of \(\frac{7}{8}\). Hence computing one conditional expectation takes us \(\mathcal{O}(m\cdot 1)\) time.</p><h4 id=bringing-it-together>Bringing it together</h4><p>Aforementioned properties can now directly be turned into a deterministic greedy algorithm.</p><pre tabindex=0><code>for i = 1 to n:
  M_0 = E[M|l_1 = a_1, ... ,l_i-1 = a_i-1, l_i = 0]
  M_1 = E[M|l_1 = a_1, ... ,l_i-1 = a_i-1, l_i = 1]
  if M_0 &gt;= M_1:
    a_i = 0
  else:
    a_i = 1
return a_1, ..., a_n
</code></pre><p>We obtain a literal assignment satisfying at least \(\frac{7}{8}\) of the clauses in \(\mathcal{O}(mn)\) time.</p><h3 id=closing-notes>Closing notes</h3><p><a href="https://dl.acm.org/citation.cfm?doid=502090.502098">Håstad</a> showed that finding a polynomial time algorithm satisfying more than \(\frac{7}{8}\) of the clauses for satisfiable formulas is impossible, unless&mldr; well unless \(\mathcal{P} = \mathcal{NP}\). Hence, given that the latter does not hold and a polynomial time constraint, this elegant algorithm is optimal with respect to fulfilling as many clauses as possible.</p><p>I discovered this through Angelika Steger&rsquo;s graduate course <a href=https://www.cadmo.ethz.ch/education/lectures/HS18/RandAlg/index.html>Randomized Algorithms and Probabilistic Methods</a> at ETH. It is among the top three courses I have attended during all of my studies.</p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/math>math</a></li></ul></nav></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/kklein title=GitHub><i data-feather=github></i></a><a class=soc href=https://twitter.com/kevkle title=Twitter><i data-feather=twitter></i></a></div><div class=footer-info>2022 © Kevin Klein |</div></footer><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-122721044-1','auto'),ga('send','pageview'))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script>feather.replace()</script></div></body></html>