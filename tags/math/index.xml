<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>math on Blog</title><link>https://kevinkle.in/tags/math/</link><description>Recent content in math on Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Â© Kevin Klein</copyright><lastBuildDate>Wed, 03 May 2023 07:38:07 +0200</lastBuildDate><atom:link href="https://kevinkle.in/tags/math/index.xml" rel="self" type="application/rss+xml"/><item><title>How Much Is a Hundred Bucks?</title><link>https://kevinkle.in/posts/2023-04-23-100_dollars_worth/</link><pubDate>Wed, 03 May 2023 07:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2023-04-23-100_dollars_worth/</guid><description>The calculator accompanying this post can be found here.
The value of time, the value of money There are various ways of trying to assess the value of something. For instance, one might define the value of a resource by how much someone else is willing to pay. In the domain of time we could ask the question of the hourly rate someone is willing to pay for our time/labor and consider that the value of our time.</description></item><item><title>How Many Inner Nodes</title><link>https://kevinkle.in/posts/2023-01-06-inner_nodes/</link><pubDate>Sun, 08 Jan 2023 07:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2023-01-06-inner_nodes/</guid><description>Motivation Assume you are working with a decision tree which as been constructed by a learning algorithm. Further assume that some of the leaves are relevant while others aren&amp;rsquo;t. For example, it could be that the outcome modeled in leaves corresponds to the probability of a successful surgery. In order to use this model for prediction/inference in production, one might imagine stripping all of its unnecessary parts. To be concrete, all of the leaves with a success probability which is no sufficiently high become irrelevant: the applicant might define a threshold, e.</description></item><item><title>A Cheat Sheet for Information Theory Fundamentals in ML</title><link>https://kevinkle.in/posts/2022-12-31-information_theory/</link><pubDate>Sat, 31 Dec 2022 07:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2022-12-31-information_theory/</guid><description>All of the definitions stem from Kevin Murphy&amp;rsquo;s Probabilistic Machine Learning: An Introduction.
Many of the following primitives are defined for discrete as well as continuous random variables and distributions. Since they are analogous, I chose to only represent discrete versions for the sake of simplicity.
Entropy $$\mathbb{H}(X) = \mathbb{H}(p) := -\sum_\mathcal{X} p(X=x)\log_2 p(X=x)$$ Cross-entropy $$\mathbb{H}(p, q) := -\sum_\mathcal{X} p(X=x) \log_2 q(X=x)$$ Joint entropy $$\mathbb{H}(X, Y) := -\sum_{\mathcal{X}, \mathcal{Y}} p(X=x, Y=y)\log_2 p(X=x, Y=y)$$ Conditional entropy $$\begin{aligned} \mathbb{H}(X|Y) &amp;amp;:= \mathbb{E}_{p(X)}[\mathbb{H}(p(Y|X))] \\ &amp;amp;= \mathbb{H}(X, Y) - \mathbb{H}(X) \end{aligned}$$ Chain rule for entropy $$\mathbb{H}(X_1, X_2, \dots, X_n) = \sum_{i=1}^n \mathbb{H}(X_i|X_1, \dots X_{i-1}) $$ KL-divergence (a.</description></item><item><title>Of Seals, Dogs and Dolphins with Python</title><link>https://kevinkle.in/posts/2022-09-25-collage/</link><pubDate>Sun, 02 Oct 2022 07:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2022-09-25-collage/</guid><description>Every now and then I have come across prints of image collages or &amp;lsquo;mosaics&amp;rsquo;. Often it&amp;rsquo;s been ads, say a Greenpeace collage of small images portraying poorly treated puppies - that is, if you&amp;rsquo;re able to squint your eyes real hard - making up for the overall image of a happy dolphin, or so.
Anyhow! - I&amp;rsquo;ve almost always found these collages to be aesthetically displeasing. At the same time they somehow caught and kept my attention.</description></item><item><title>Who With Whom, What With What</title><link>https://kevinkle.in/posts/2022-06-21-friends/</link><pubDate>Wed, 22 Jun 2022 07:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2022-06-21-friends/</guid><description>The following is an illustration of a graph optimization problem. I chose to instantiate it with a decision making process revolving around a social setting. Yet I hope its general applicability comes across.
Setup Let&amp;rsquo;s say we have the opportunity to invite people to an event. The pool of candidates is comprised of five people: Tony, Paulie, Silvio, Furio and Vito. What we care about is for the attendants to have a good time - which we assume to be purely determined by who else is attending.</description></item><item><title>Inference</title><link>https://kevinkle.in/posts/2022-06-06-inference/</link><pubDate>Sun, 05 Jun 2022 07:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2022-06-06-inference/</guid><description>I had long felt a slight discomfort with the word &amp;lsquo;inference&amp;rsquo;. Many people seemed to use it with confidence and the expectation that it would refer to a precise notion. And while it is not uncommon for a word to have different meaning in different contexts, it took me a while to figure out what the different meaning and what the different contexts are in the example of &amp;lsquo;inference&amp;rsquo;. Some other people seem to be a little confused as well.</description></item><item><title>Sudoku #3: Poor Man's RL</title><link>https://kevinkle.in/posts/2022-01-09-sudoku_rl/</link><pubDate>Sun, 13 Feb 2022 08:00:07 +0200</pubDate><guid>https://kevinkle.in/posts/2022-01-09-sudoku_rl/</guid><description>Previously, I&amp;rsquo;ve outlined some ideas on how to solve Suduko puzzles. One revolved around depth-first search in trees, one around linear programming. This time I tried my luck with a more adventurous, data-driven approach: Reinforcement Learning. First things first: the approach only works well for 4x4 grids - not for the typical 9x9. Please note that the approached subsequently outlined couldn&amp;rsquo;t be further from a recommendable approach to solving Sudoku puzzles.</description></item><item><title>Sudoku #2: Linear Programming</title><link>https://kevinkle.in/posts/2021-03-14-sudoku_lp/</link><pubDate>Sun, 14 Mar 2021 11:00:07 +0200</pubDate><guid>https://kevinkle.in/posts/2021-03-14-sudoku_lp/</guid><description>Abstract As discussed in this previous post, a given Sudoku puzzle can be modeled in a multitude of ways. Different algorithms ought to still lead to the same solution, since a correct Sudoku puzzle comes with a unique solution. In this post we&amp;rsquo;ll rely on optimization&amp;rsquo;s poster child: Linear programming.
Idea A linear program, subsequently referred to as LP, comes with three central building blocks:
An objective function Typically many variables, each, a priori, non-integer numbers Linear constraints on the variables Since, thanks to our assumption of a well-posed puzzle we know that:</description></item><item><title>Sudoku #1: Depth-First Search</title><link>https://kevinkle.in/posts/2021-02-28-sudoku_dfs/</link><pubDate>Sun, 28 Feb 2021 19:00:07 +0200</pubDate><guid>https://kevinkle.in/posts/2021-02-28-sudoku_dfs/</guid><description>Abstract Solving a Sudoku board can be done in many ways. Let&amp;rsquo;s explore a depth-first search approach in this post.
Idea Depth-first search or DFS implies (at least) two things:
a graph, or rather, a tree a search This bares the question: What&amp;rsquo;s the tree and what are we searching for?
The tree A tree must consist of nodes and edges. We define the nodes to represent a &amp;lsquo;state&amp;rsquo; of the game.</description></item><item><title>Markov Random Field Image Denoising</title><link>https://kevinkle.in/posts/2020-03-27-mrf/</link><pubDate>Sun, 26 Apr 2020 09:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2020-03-27-mrf/</guid><description>This post describes the application of a Probabilistic Graphical Model for simple image denoising, as suggested in Bishop&amp;rsquo;s chapter 8.3.8. I will attempt to introduce some notions for general context without derivations. Googling will lead to plenty of useful resources. On an introductory level, I enjoyed Daphne Koller&amp;rsquo;s class.
Context: Probabilistic Graphical Models Probabilistic Graphical Models are tools to model and express conditional independencies among a set of random variables.</description></item><item><title>Invitations</title><link>https://kevinkle.in/posts/2020-03-02-invitations/</link><pubDate>Mon, 02 Mar 2020 09:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2020-03-02-invitations/</guid><description>Scenario and goal You are the host of an event for which there are \(n\) candidates to be invited and you possess sufficient resources to host all candidates. Let the nature of the event be such that it seems personally attached to you, yet also fairly polarizing in its experience. In other words:
some invitees will have a desire to decline invitees with a desire to decline will feel bad about declining some non-invitees will have a desire to attend non-invitees with a desire to attend will feel bad about not being able to attend Moreover, one might weigh the ambitions wrt non-desiring invitees and desiring non-invitees differently.</description></item><item><title>Sort-of-art?</title><link>https://kevinkle.in/posts/2019-09-27-lena/</link><pubDate>Fri, 27 Sep 2019 08:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2019-09-27-lena/</guid><description>Recently, my friend Theo pointed me towards his amazing idea and execution of programming &amp;lsquo;paintings&amp;rsquo; with the help of randomization.
I was fairly taken by the idea of programming with randomization to create sort-of-art. I kept thinking about cellular automata but reckoned that they often created outcomes that looked:
overly structured in general busy, hard and cold in detail. I figured that the latter could maybe be tackled by applying lots of smoothing, both on input and output image.</description></item><item><title>Breaking Pills: An Intuition</title><link>https://kevinkle.in/posts/2019-09-23-breaking_pills/</link><pubDate>Mon, 23 Sep 2019 09:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2019-09-23-breaking_pills/</guid><description>Motivation Recently, Mark Dominus&amp;rsquo; post on breaking pills appeared on Hacker News. I was surprised to see the author not provide a closed-form and wanted to extend on some of the comments indicating a relation to the harmonic numbers.
Problem Formulation Starting off with a bowl of \(n\) whole pills, you draw from the bowl uniformly at random according to the following mechanism: if the drawn pill is whole, you put back one half of it; if the drawn pill is half, you don&amp;rsquo;t put back anything.</description></item><item><title>A Beautiful, Derandomized Algorithm</title><link>https://kevinkle.in/posts/2019-01-22-favourite-algorithm/</link><pubDate>Mon, 21 Jan 2019 16:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2019-01-22-favourite-algorithm/</guid><description>Abstract Appreciating the simplicity of powerful randomized algorithms comes easy to me. In the following I will try to describe a naive algorithm relying on a randomized argument. What I find particularly remarkable is that it can easily parry the critique I am so often encountered with when cherishing randomized algorithms: it can trivially be turned deterministic.
MAX-3SAT Is a boolean satisfiability problem. Given a formula in 3-CNF, the goal to satisfy as many clauses as possible.</description></item><item><title>Matrix Factorization in Collaborative Filtering</title><link>https://kevinkle.in/posts/2018-07-10-cf-mf/</link><pubDate>Tue, 10 Jul 2018 16:38:07 +0200</pubDate><guid>https://kevinkle.in/posts/2018-07-10-cf-mf/</guid><description>The Task We are given a set of tuples representing user-movie ratings, \(\mathcal{R} = \{(i,j,r)\}\), where \(i\) is the user index, \(j\) is the movie index and \(r \in [0, \dots, 5]\) the rating. In the most common scenarios, only a fraction of all user-movie pairs are rated, hence we can talk about sparse data. In other words, most users have only rated a few movies. Now we want to obtain sensible predictions for movies a user has not rated.</description></item></channel></rss>